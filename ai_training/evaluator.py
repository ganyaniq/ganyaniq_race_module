# ğŸ“ MODÃœL: evaluator.py
# ğŸ§¾ AÃ‡IKLAMA: Alfonso'nun tahmin sonuÃ§larÄ±nÄ±n doÄŸruluÄŸunu Ã¶lÃ§er, metrik raporlar Ã¼retir

from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
import pandas as pd


def degerlendir_model(y_true, y_pred):
    """
    GerÃ§ek deÄŸerler ve tahminlere gÃ¶re performans Ã¶lÃ§er.
    :param y_true: GerÃ§ek etiketler
    :param y_pred: Modelin tahmin ettiÄŸi etiketler
    :return: Performans metrikleri sÃ¶zlÃ¼ÄŸÃ¼
    """
    try:
        acc = accuracy_score(y_true, y_pred)
        cm = confusion_matrix(y_true, y_pred)
        report = classification_report(y_true, y_pred, output_dict=True)

        return {
            "accuracy": acc,
            "confusion_matrix": cm.tolist(),
            "classification_report": report
        }
    except Exception as e:
        return {"hata": f"DeÄŸerlendirme yapÄ±lamadÄ±: {e}"}


# Ã–rnek test:
# y_true = [1, 0, 1, 1, 0]
# y_pred = [1, 0, 0, 1, 0]
# print(degerlendir_model(y_true, y_pred))
